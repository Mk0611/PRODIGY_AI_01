from transformers import GPT2LMHeadModel, GPT2Tokenizer 


# Load pre-trained model and tokenizer 
model = GPT2LMHeadModel.from_pretrained("gpt2") 
tokenizer = GPT2Tokenizer.from_pretrained("gpt2") 
from datasets import load_dataset 

# Load your custom dataset 
dataset = load_dataset('text', data_files={'train': 'path/to/your/dataset.txt'}) 

# Tokenize the dataset 
def tokenize_function(examples): 
    return tokenizer(examples['text'], truncation=True) 

tokenized_datasets = dataset.map(tokenize_function, batched=True) 
from transformers import Trainer, TrainingArguments 

training_args = TrainingArguments( 
    output_dir="./results", 
    evaluation_strategy="epoch", 
    learning_rate=2e-5, 
    per_device_train_batch_size=4, 
    num_train_epochs=3, 
    weight_decay=0.01, 
) 

trainer = Trainer( 
    model=model, 
    args=training_args, 
    train_dataset=tokenized_datasets['train'], 
) 

trainer.train() 
prompt = "Once upon a time" 
input_ids = tokenizer.encode(prompt, return_tensors='pt') 

# Generate text 
output = model.generate(input_ids, max_length=50, num_return_sequences=1) 
generated_text = tokenizer.decode(output[0], skip_special_tokens=True) 

print(generated_text)
